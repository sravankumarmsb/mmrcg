# -*- coding: utf-8 -*-
"""mmrcs_clippretrained_flickr30k.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19YazES_TR_I6ysP8ZD-uCUyy7RMDgADs
"""

import os
import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
import zipfile
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt

import logging
from tqdm import tqdm
import pandas as pd
import json
from collections import defaultdict
import shutil
import streamlit as st
import configparser

# Load configuration from config.ini
config = configparser.ConfigParser()
config.read("config.ini")

# Read base path from config file
base_path = config.get("paths", "base_path_30k")
project_path = config.get("paths", "project_path")


# Paths
images_path = os.path.join(base_path, "flickr30k_images")
captions_file = os.path.join(base_path, "captions.txt")
images_embeddings_file = os.path.join(base_path, "30k_pretrain_image_embeddings.pt")
captions_embeddings_file = os.path.join(base_path, "30k_pretrain_caption_embeddings.pt")

def setup_logging(log_directory=base_path, log_file="mmrcs30k.log"):
    """
    Set up logging configuration with both console and file handlers.
    Args:
        log_directory (str): Directory where the log file will be saved.
        log_file (str): The name of the log file.
    Returns:
        logger: Configured logger object.
    """
    # Ensure the log directory exists
    if not os.path.exists(log_directory):
        os.makedirs(log_directory)

    # Full log file path
    log_path = os.path.join(log_directory, log_file)
    # Create a custom logger
    logger = logging.getLogger()
    # First, remove all existing handlers to avoid duplication
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    # Set the logging level to INFO
    logger.setLevel(logging.INFO)
    # Create handlers for file and console output
    file_handler = logging.FileHandler(log_path)
    console_handler = logging.StreamHandler()
    # Set the log level for handlers
    file_handler.setLevel(logging.INFO)
    console_handler.setLevel(logging.INFO)
    # Create a formatter and set it for both handlers
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    # Add handlers to the logger
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    return logger
logger = setup_logging()

#!rm -rf /content/flixke8k_dataset

# Load the pre-trained CLIP model and processor
device = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(device)
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

def load_captions(df):
    df["image"] = df["image"].str.strip()
    df["caption"] = df["caption"].str.strip()
    captions_dict = defaultdict(list)  # Dictionary to store image -> list of captions
    for _, row in df.iterrows():
        image_name = row["image"]
        caption_text = row["caption"]
        captions_dict[image_name].append(caption_text)  # Store caption under the image

    return captions_dict  # Return a dictionary, not a list

captions_csv = os.path.join(base_path, "captions.csv")
dataframe = pd.read_csv(captions_csv)
captions = load_captions(dataframe)

@st.cache_data
def load_embeddings_cached(embeddings_file):
    """
    Load embeddings and corresponding filenames from a single file.

    Args:
        embeddings_file (str): Path to the embeddings file.

    Returns:
        filenames (list): List of filenames.
        embeddings (Tensor): Tensor containing all embeddings.
        embeddings_dict (dict): Mapping from embeddings to filenames.
    """
    #embeddings = torch.load(embeddings_file)
    embeddings = torch.load(embeddings_file, weights_only=True)

    filenames_file = embeddings_file.replace('.pt', '_filenames.json')
    with open(filenames_file, 'r') as f:
        filenames = json.load(f)

    #embeddings_dict = {str(emb.tolist()): filename for emb, filename in zip(embeddings, filenames)}

    embeddings_dict = {}
    for emb, filename in zip(embeddings, filenames):
      key = str(filename)  # Use filename as key
      if key in embeddings_dict:
        embeddings_dict[key].append(emb.tolist())  # Append to list
      else:
        embeddings_dict[key] = [emb.tolist()]  # Create a list for multiple embeddings

    return filenames, embeddings, embeddings_dict

# Load image and caption embeddings
image_filenames_list, image_embeddings_list, image_embeddings_dict = load_embeddings_cached(images_embeddings_file)
captions_filenames_list, captions_embeddings_list, captions_embeddings_dict = load_embeddings_cached(captions_embeddings_file)

# Function to get embeddings for text using CLIP
def get_query_embedding(query_string):
    inputs = processor(text=query_string, return_tensors="pt", padding=True, truncation=True).to(device)
    with torch.no_grad():
        text_embedding = model.get_text_features(**inputs)
    return text_embedding / torch.norm(text_embedding)  # Normalize

# Function to get embeddings for Image using CLIP
def get_image_embedding(image_path):
    image = Image.open(image_path).convert("RGB")  # Ensure image is in RGB mode
    inputs = processor(images=image, return_tensors="pt").to(device)
    with torch.no_grad():
        image_embedding = model.get_image_features(**inputs)
    return image_embedding / torch.norm(image_embedding)

# Function to display top images with their ranks and scores
def display_images(top_images, images_path):
    # Create a figure with enough subplots to display images
    num_images = len(top_images)
    cols = 3  # You can change this based on how many images you want in a row
    rows = (num_images + cols - 1) // cols  # Calculate rows needed based on the number of images

    plt.figure(figsize=(20, 3 * rows))  # Adjust the size of the figure based on number of images

    for i, img_info in enumerate(top_images):
        # Ensure the filename has the .jpg extension (if not, add it)
        filename = img_info['item']

        if not filename.endswith('.jpg'):
          filename = filename.replace('.pt', '.jpg')
        # Path to the image file
        image_path = os.path.join(images_path, filename)

        # Open the image
        try:
            img = Image.open(image_path)
            ax = plt.subplot(rows, cols, i + 1)
            ax.imshow(img)
            ax.set_title(f"Rank {i + 1}: {filename} (Score: {img_info['score']:.4f})")
            ax.axis('off')
        except Exception as e:
            logger.error(f"Error loading image {filename}: {e}")

    plt.tight_layout()
    plt.show()

def display_iteam(image_path):
  img = Image.open(image_path)
  plt.figure(figsize=(2, 2))
  plt.imshow(img)
  plt.axis("off")  # Hide axes
  plt.show()

def search_top_matches(query_input, embeddings_list, items_list, top_n, get_embedding_fn):
    """
    Generic function to search for top N matches based on cosine similarity.

    Parameters:
    - query_input (str or image_path): The input query (text caption or image path).
    - embeddings_list (list of Tensors): List of stored embeddings (image or caption).
    - items_list (list): List of items (image filenames or text captions).
    - top_n (int): Number of top matches to return.
    - get_embedding_fn (function): Function to get the embedding (e.g., get_image_embedding or get_query_embedding).

    Returns:
    - List of dictionaries with "item" and "score" keys.
    """
    # Get the query embedding and normalize it
    query_embedding = get_embedding_fn(query_input).to(device)
    query_embedding = query_embedding / torch.norm(query_embedding)

    # Convert list of embeddings into a tensor for batch processing
    embeddings_list = [torch.tensor(embedding) if not isinstance(embedding, torch.Tensor) else embedding for embedding in embeddings_list]
    embeddings_tensor = torch.stack(embeddings_list).to(device)

    # Normalize all stored embeddings
    embeddings_tensor = embeddings_tensor / torch.norm(embeddings_tensor, dim=1, keepdim=True)

    # Compute cosine similarity in batch
    similarities = torch.matmul(embeddings_tensor, query_embedding.T).squeeze()

    # Get indices of top N matches
    #top_indices = torch.argsort(similarities, descending=True)[:top_n]

    # Retrieve top matching items
    #top_matches = [{"item": items_list[i], "score": similarities[i].item()} for i in top_indices]

    # Get indices of top N+1 matches (to account for removing query itself)
    top_indices = torch.argsort(similarities, descending=True)

    # Retrieve top matching items, excluding the query image itself and scores >= 1
    top_matches = []
    query_index = items_list.index(query_input) if query_input in items_list else None  # Find query index if it exists

    for i in top_indices:
        if len(top_matches) >= top_n:
            break  # Stop once we have enough matches
        if query_index is not None and i == query_index:
            continue  # Skip the query image itself
        if similarities[i].item() >= 1:
            continue  # Skip matches with scores >= 1

        top_matches.append({"item": items_list[i], "score": similarities[i].item()})


    return top_matches

top_n = 1
def imageToCaptions(image):
  image_path = os.path.join(project_path,"uploaded_files", image.name)
  logger.info(f"Top {top_n} images:")
  top_captions = search_top_matches(image_path,captions_embeddings_list,captions_filenames_list,top_n,get_image_embedding)
  caption = top_captions[0]['item']
  base_name, number = caption.rsplit('_', 1)    
  match = dataframe[(dataframe['image'] == base_name) & (dataframe['caption_number'] == int(number))]
  # Return the 'caption' column of the matched row
  caption_text = match['caption'].iloc[0] if not match.empty else None
  return caption_text
  
def captionToImage(caption):
  top_images = search_top_matches(caption, image_embeddings_list, image_filenames_list,top_n,get_query_embedding)
  image_name=top_images[0]['item']  
  return image_name
  
def imageToImage(image):
  image_path = os.path.join(project_path,"uploaded_files", image.name)
  top_images = search_top_matches(image_path, image_embeddings_list, image_filenames_list,top_n,get_image_embedding)
  image_name=top_images[0]['item']  
  return image_name